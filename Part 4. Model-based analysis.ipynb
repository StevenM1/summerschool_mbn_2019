{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4. Model-based analysis\n",
    "\n",
    "In the previous part, we pre-processed the fmri data, we fit the behavioral model, and extracted signal from each participant's striatum. Now, we're ready to do the final step: the model-based analysis, to see if differences in signal in striatum due to a `speed` cue versus an `accuracy` cue correlate with threshold differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "\n",
    "import glob\n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We start out by loading all extracted striatum signal of all participants. Using the following line of code, you can make a list of all `.txt`-files that were generated before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fns = glob.glob('/data/extracted_signals/extracted_timeseries/_mask_*_subject_id_*/_extract*/*.txt')\n",
    "fns[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the file names contains the subject id, the applied mask, and the run/block number. We later want to extract these, and for this we can use a regular expression. It's a bit of an 'art' to create these, but you want something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = re.compile('.*/_mask_(?P<mask_name>.*)_subject_id_(?P<subj_idx>.*)/_extract_mean_ts[0-9]/sub-[0-9]*_task-SAT_run-(?P<block>[0-9])_bold_space-MNI152NLin2009cAsym_preproc_ts.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize what reg does, you can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fn in fns[:5]:\n",
    "    print(fn)\n",
    "    print(reg.match(fn).groupdict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) What does the call `reg.match(fn).groupdict()` do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An especially useful library is [pandas](http://pandas.pydata.org/), which allows you to manipulate data in an `R`-like DataFrame. Next, I'll combine all neural data of all subjects in a single DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty list first\n",
    "df = []\n",
    "\n",
    "# loop over .txt-files, adding signal row-by-row\n",
    "for fn in fns:\n",
    "    d = reg.match(fn).groupdict()  # here, we extract the paths\n",
    "    d['signal'] = np.loadtxt(fn)\n",
    "    df.append(d)\n",
    "\n",
    "df = pd.DataFrame(df)  # here, we convert the list to a DataFrame\n",
    "df['block'] = df['block'].astype(int)\n",
    "df['subj_idx'] = df['subj_idx'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Have a look at the head (first few lines) of the dataframe, what do you see? Can you plot the signal for a few subjects?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A look at design matrices\n",
    "Crucial to the GLM is the design matrix. The general idea is that you, as a researcher, specify when in the experiment each 'event' took place (i.e., each cue, stimulus, perhaps responses - anything that you think may elicit a BOLD response). Then, you convolve these events with the canonical hemodynamic response function. The result of this convolution is the _predicted_ timeseries.\n",
    "\n",
    "`Nipy` is a Python package that allows you to easily create a design matrix. Before turning to the actual experiment, let's make a dummy design matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some functions\n",
    "from nipy.modalities.fmri import design_matrix, experimental_paradigm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose we create a dummy experiment with 2 conditions. Each condition occurs three times in the experiment\n",
    "conditions = ['condition1'] * 3 + ['condition2'] * 3\n",
    "\n",
    "# Then, we specify *when* (time in seconds from experiment onset, start counting at 0) events took place (collapse over conditions here)\n",
    "onsets = [5, 25 , 35, 15, 30, 40]\n",
    "\n",
    "# You need to know the TR of the scanning sequence\n",
    "tr = 2.0\n",
    "\n",
    "# Create a vector containing all the time points at which you have a volume (i.e. 'scan'/'image')\n",
    "frametimes = np.arange(0, 50, tr)\n",
    "\n",
    "# Define the type of hemodynamic response function (hrf) you want to use\n",
    "hrf_model = 'Canonical'\n",
    "\n",
    "# Set-up the paradigm\n",
    "paradigm =  experimental_paradigm.BlockParadigm(con_id=conditions, \n",
    "                                                onset=onsets,\n",
    "                                                duration=[[1.]] * len(conditions))  \n",
    "# Note that duration here is *not* the TR, but the duration of the events\n",
    "\n",
    "X, names = design_matrix.dmtx_light(frametimes, paradigm, hrf_model=hrf_model, hfcut=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Have a look at the newly created variable X. What is its shape (how many columns & rows are there)? Also plot X. What are each of the lines?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's make the real design matrices\n",
    "In BIDS-format, the information about the events in the experiment is always placed together with the BOLD-nifti. That is, you can find it here:\n",
    "\n",
    "`/data/bids/sub-<subject_id>/func/sub-<subject_id>_task-SAT_run-<run_idx>_events.tsv'`  \n",
    "\n",
    "(.tsv is a tab-separated file, very much like a csv)\n",
    "\n",
    "Let's load the events of a single subject (let's take 548) and a single run/block (block 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_idx = 548\n",
    "block = 1\n",
    "mask = 'STR_L'\n",
    "\n",
    "events = pd.read_csv('/data/bids/sub-%d/func/sub-%d_task-SAT_run-%d_events.tsv' %(subj_idx, subj_idx, block), sep='\\t')\n",
    "events = events[pd.notnull(events.event_type)]  # remove null trials\n",
    "\n",
    "events.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`events` is now a DataFrame with four columns: onset (when in the experiment the event started), duration, weight (you can forget about this for this experiment, but this is what you would adjust if you have a parametric design), and event_type.\n",
    "\n",
    "There are 4 event types in this run: a speed cue, an acc(uracy) cue, stimulus left, stimulus right. All events took 2 seconds. Now, turning this into a design matrix..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = events.event_type.tolist()\n",
    "onsets = events.onset.tolist()\n",
    "durations = events.duration.tolist()\n",
    "\n",
    "tr = 1.994\n",
    "frametimes = np.arange(0, 706, tr)\n",
    "hrf_model = 'Canonical'\n",
    "\n",
    "\n",
    "paradigm =  experimental_paradigm.BlockParadigm(con_id=conditions, \n",
    "                                                onset=onsets,\n",
    "                                                duration=[[2.]] * len(conditions))\n",
    "\n",
    "# Set up GLM\n",
    "X, names = design_matrix.dmtx_light(frametimes, paradigm, hrf_model=hrf_model, drift_model='blank')\n",
    "X = pd.DataFrame(X, columns=names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, plot X. What are the 'conditions' here, and what are the 'onsets'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X)\n",
    "\n",
    "# this line below re-sizes the plot to aid visualization\n",
    "plt.gcf().set_size_inches((20, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X is what we _expect_ a voxel does *if* it responds to the defined events. Now, in order to assess whether or not striatum responds to these events, we fit the data to our GLM, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "Y = df[(df.subj_idx == subj_idx) & (df.block == block) & (df.mask_name == mask)].iloc[0].signal\n",
    "model = sm.OLS(Y, X)\n",
    "r = model.fit()\n",
    "r.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Interpret these results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to fit the GLM for all participants & blocks & masks. In the following function, we create the design matrix for each subject/mask/block, and fit the GLM.\n",
    "\n",
    "6) Create a function that receives the subj_idx, block, and mask as arguments, and returns the beta-values of a fitted glm. (Most of this is copying the previous cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_glm(subj_idx, block, mask):\n",
    "    \n",
    "    # your code here. This is a bit tricky if you've never coded before, so you may want to peek at the answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your function is correct, running the following cell should give the beta values for acc (0.4672429), speed (1.563291), and a constant (389.445299)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_glm(548, 1, 'STR_R')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for fn in fns:\n",
    "    subj_idx = int(reg.match(fn).groupdict()['subj_idx'])\n",
    "    block = int(reg.match(fn).groupdict()['block'])\n",
    "    for mask in ['STR_L', 'STR_R']:\n",
    "        r = fit_glm(subj_idx, block, mask)\n",
    "        \n",
    "        for condition in ['acc', 'speed']:\n",
    "            d = {}\n",
    "            d['subj_idx'] = subj_idx\n",
    "            d['mask_name'] = mask\n",
    "            d['block'] = block\n",
    "            d['condition'] = condition\n",
    "            d['value'] = r[condition]\n",
    "            results.append(d)\n",
    "\n",
    "results = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the `results` DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, in the above cells we fitted a GLM for each subject & block & mask combination. Next, we want to combine these results with the behavioral results. In order to do this, it's useful to first transform the shape of the data frame a little bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_results = results.pivot_table(values='value', columns=['condition', 'mask_name'], index=['subj_idx'])\n",
    "neural_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) What did the cell above do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to load the behavioral parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "behavioral_results = pd.read_csv('/data/behavior_fits/parameters_per_subject.csv')\n",
    "behavioral_results.rename(columns={'Unnamed: 0': 'subj_idx'}, inplace=True)\n",
    "\n",
    "# Get only threshold\n",
    "behavioral_results = behavioral_results[['subj_idx', 'a.spd', 'a.acc']]\n",
    "behavioral_results = behavioral_results.set_index('subj_idx')\n",
    "behavioral_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can combine the behavioral & neural data as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = pd.merge(neural_results, behavioral_results, suffixes=('_fmri', '_threshold'), left_index=True, right_index=True)\n",
    "combined_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) Can you plot the a scatterplot with:\n",
    "- on the x-axis the difference between threshold in accuracy and speed trials? (acc - speed)\n",
    "- on the y-axis the difference in striatum activation after an accuracy cue and a speed cue (acc - speed);\n",
    "\n",
    "Do this for left & right striatum separately.\n",
    "One function you could use is seaborn's [jointplot](https://seaborn.pydata.org/generated/seaborn.jointplot.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here for right striatum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here for left striatum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you see? Is there a relation between striatal activation and threshold setting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You've reached the end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
