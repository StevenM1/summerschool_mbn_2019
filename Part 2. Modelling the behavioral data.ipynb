{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. Modelling the behavioral data\n",
    "\n",
    "In Part 1, we had a look at options for preprocessing fMRI data. Now, let's turn to the other essential ingredient of model-based fMRI-analyses: modelling behavioral data.\n",
    "\n",
    "There are Python packages that allows you to easily fit decision-making models such as the drift diffusion model (DDM). Most notably, [`hddm`](http://ski.clps.brown.edu/hddm_docs/) developed at Brown University is a widely used package.\n",
    "\n",
    "However, `DMC` (as used in the tutorials earlier this week) has some advantages. For example, `DMC` offers much more models (including a multitude of LBA-model variants, stop-signal models and ex-Gaus). So you might want to use `DMC` to fit a model, and then use `Nipype` to create your fMRI-processing pipeline.\n",
    "\n",
    "So, we're just going to use `DMC` to fit the behavioral data for now. Note that _this_ notebook you're looking at right now (`Part 2. Modelling the behavioral data`) is actually _not_ an interactive Python notebook ('.ipynb'), but it is an interactive **R** notebook ('.irnb')! That means that instead of running Python code, we can run R-code in here.\n",
    "\n",
    "For example, to draw some random numbers from a normal distribution (4,1), we can use this R-function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnorm(10, 4, 1)\n",
    "\n",
    "# note further that the help-function works as usual:\n",
    "# ?rnorm\n",
    "\n",
    "# and plotting is done in-line\n",
    "plot(1:10, 1:10, main='In line plot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So let's use DMC to fit the behavioral data\n",
    "\n",
    "First, set-up some directories, so R knows where to find DMC, the data, and where to store the results of these analyses (the 'res(ults)Dir')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projDir <- '/home/neuro/nipype_tutorial'\n",
    "dataDir <- file.path('/data', 'bids')\n",
    "resDir <- file.path('/data', 'behavior_fits')\n",
    "dir.create(resDir, showWarnings=FALSE)\n",
    "dmcDir <- file.path(projDir, 'DMC-MBN2019')\n",
    "\n",
    "# Load dmc, load the DDM model\n",
    "setwd(dmcDir)\n",
    "source('dmc/dmc.R')\n",
    "source('dmc/models/DDM/ddm.R')\n",
    "library(rtdists)\n",
    "setwd(projDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the data, and reformat to a DMC-approved form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "dat <- read.csv(file.path(dataDir, 'behavior.tsv'), sep='\\t')\n",
    "\n",
    "# Remove unnecessary columns, rename according to DMC's wishes\n",
    "dat <- dat[,c('pp', 'stimulus', 'cond', 'response', 'RT')]\n",
    "colnames(dat) <- c('s', 'S', 'cue', 'R', 'RT')\n",
    "\n",
    "# Remove all extremely fast responses (<.15s)\n",
    "dat <- dat[dat$RT>.15,]\n",
    "\n",
    "# Ensure the following columns are of type factor, and rename the stimulus/response factors to S1/S2 and R1/R2\n",
    "dat$s <- factor(dat$s)\n",
    "dat$cue <- factor(dat$cue, levels=c('spd', 'acc'))\n",
    "levels(dat$S) <- c('S1', 'S2')\n",
    "levels(dat$R) <- c('R1', 'R2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The next step is to set up the model specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set-up model: specify what 'factors' are in the experiment\n",
    "factors <- list(S=c('S1', 'S2'), cue=c('spd', 'acc'))\n",
    "responses <- c(\"R1\", \"R2\")\n",
    "match.map = list(M=list(S1=\"R1\", S2=\"R2\"))  # Specify which response is 'correct' for which stimulus\n",
    "\n",
    "# set-up p.map. This tells DMC which parameters should vary across which conditions. \n",
    "# Note that we're allowing v, a, and t0 to vary across cue-condition\n",
    "p.map <- list(a='cue',\n",
    "              v='cue',\n",
    "              t0='cue',\n",
    "              z='1',\n",
    "              sz='1',\n",
    "              sv='1',\n",
    "              st0='1',\n",
    "              d='1')\n",
    "\n",
    "# Let's not estimate between-trial variabilities. Further, fix z to 0.5 (= 0.5 * a)\n",
    "constants <- c(st0=0,\n",
    "               sz=0,\n",
    "               sv=0,\n",
    "               d=0,\n",
    "               z=0.5)\n",
    "\n",
    "# And this is our model\n",
    "model <- model.dmc(constants=constants,\n",
    "                   match.map=match.map,\n",
    "                   factors=factors,\n",
    "                   responses=responses,\n",
    "                   p.map = p.map,\n",
    "                   type=\"norm\")\n",
    "\n",
    "# combine model and data\n",
    "data.model <- data.model.dmc(data=dat, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks good! Two thresholds, drift rates, non-decision times (one for speed, one for accuracy) \n",
    "\n",
    "##### Now, we need to set-up priors for these parameters.\n",
    "\n",
    "Let's use some mildly informed priors\n",
    "\n",
    "- v ~ TN(2.5, 2.5)  truncated lower at 0\n",
    "- a ~ TN(1, 2)   truncated lower at 0\n",
    "- t0 ~ TN(0.25, .25)  truncated lower at 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.prior <- prior.p.dmc(\n",
    "  dists = rep('tnorm', 6),\n",
    "  p1=c(v.spd=2.5, v.acc=2.5, a.spd=1.00, a.acc=1.00, t0.spd=0.25, t0.acc=0.25),  # these are the means\n",
    "  p2=c(v.spd=2.5, v.acc=2.5, a.spd=2, a.acc=2, t0.spd=0.25, t0.acc=0.25),  # and these the SDs\n",
    "  lower=c(0, 0, 0, 0, 0, 0),  # lower bound is 0 for v, a, t0\n",
    "  upper=c(NA, NA, NA, NA, NA, NA)  # no upper bound\n",
    ")\n",
    "\n",
    "# What do they look like?\n",
    "par(mfrow=c(3,2))\n",
    "for(i in 1:length(p.prior)) plot.prior(i, p.prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we're ready to start sampling!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hsamples0 <- h.samples.dmc(nmc=250, p.prior, data.model, thin=1)\n",
    "\n",
    "# start with a burn-in of 250 samples with migration turned on\n",
    "hsamples1 <- h.run.dmc(hsamples0, cores=20, p.migrate=0.05, report=1)  # hsamples1 = burn-in\n",
    "# This takes a while...\n",
    "\n",
    "# Nothing stuck?\n",
    "h.pick.stuck.dmc(hsamples1, start=200)\n",
    "par(mfrow=c(2,2))\n",
    "for(i in 1:length(hsamples1)) {\n",
    "    plot.dmc(hsamples1, pll.chain=TRUE, subject = i, start=50)\n",
    "    title(paste0('Subject ', names(hsamples1)[i]))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I ran this before, there were no stuck chains and 250 burn-in iterations was sufficient. However, if you find stuck chains, you may want to add some extra burn-in samples before you continue.\n",
    "\n",
    "Otherwise, let's sample from the posterior now\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get 1000 samples from posterior\n",
    "hsamples2 <- h.run.dmc(h.samples.dmc(samples = hsamples1, nmc=1000, add=FALSE),  # don't add to burn-in but start counting from 0\n",
    "                       cores=20, p.migrate=0.00, report=1)                       # turn off migration in real sampling\n",
    "\n",
    "# Let's save these results\n",
    "save(hsamples0, hsamples1, hsamples2, file=file.path(resDir, 'samples_flat.Rdata'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB this can take quite some time - if you want to continue, you can skip this cell, and load the pre-sampled posteriors in the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load(file.path(resDir, 'samples_flat.Rdata'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inspection\n",
    "Now that the sampling is done, we need to inspect whether everything went right. MCMC is a bit error prone - chains can get stuck, for example. If sampling did not go well, you *cannot* interpret the resulting parameters. One thing you want to do is inspect Gelman's diagnostic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmdiag = h.gelman.diag.dmc(hsamples2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, you want them to be smaller than 1.1 (at most); I usually prefer values of 1.05 and below. Sub 559 seems a bit of an issue (we'll look into that below)\n",
    "\n",
    "What about the effective sample sizes? Did we sample sufficient (independent) samples from the posteriors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = effectiveSize.dmc(hsamples2)\n",
    "do.call(rbind, es)  # this prints a data frame with all effective sample sizes per participant/parameter combination\n",
    "min(do.call(rbind, es))  # prints the smallers number in the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\>500 samples is fine for now\n",
    "\n",
    "It's also important to visualize and inspect your chains. Plot the chains at the subject level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(warn=-1)  # suppresses some warnings\n",
    "for(pp in names(hsamples2)) {\n",
    "    par(mfrow=c(1,1))\n",
    "    plot.dmc(hsamples2, hyper=FALSE, subject=pp, pll.chain=TRUE)\n",
    "    mtext(paste0('Participant ', pp, ' posterior log likelihoods of all chains'), outer=TRUE, line=-1.5)\n",
    "    plot.dmc(hsamples2,hyper=FALSE,subject=pp, layout=c(4,2), density = TRUE)\n",
    "    mtext(paste0('Participant ', pp), outer=TRUE, line=-1.5)\n",
    "}\n",
    "options(warn=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you want to see are \"big hairy caterpillars\" (left columns), that are stable (no drift up or down) and with no stuck chains ('hairs' that dont move). In the right columns, you want smooth densities.\n",
    "\n",
    "There's some problems with the estimation of t0.spd of the last subject - a minority of chains are 'stuck' towards higher t0 estimates. Ideally you'd get rid of this, but since it doesn't appear to influence the posterior median much, nor seems to trade-off with other parameters (eg threshold, which is of primary interest here), I'm not too worried about this for the purposes of this tutorial. Not great though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the quality of fits? In order to visualize this, we first generate 'posterior predictives', and then plot these against the real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h.posterior.preds <- h.post.predict.dmc(hsamples2, n.post = 100, cores=4)\n",
    "plot.pp.dmc(h.posterior.preds, layout=c(2,2), x.min.max = c(0, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think of the fit?\n",
    "\n",
    "Finally, for model-based analyses in Part 4 of this tutorial, we need parameters. Let's save these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summ <- summary.dmc(hsamples2)\n",
    "\n",
    "MAP.per.sub <- do.call(rbind, lapply(summ, function(x) x$statistics[,1]))\n",
    "write.csv(MAP.per.sub, file=file.path(resDir, 'parameters_per_subject_flat.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise\n",
    "The above fit is (I think) pretty nice, especially considering that we have only 80 trials per participant. However, it's not necessarily the _best_ model; we could consider _not_ allowing v, a, or t0 to vary between conditions, and do a formal model comparison to see which model specification has the best trade-off between complexity and quality of fit (using, e.g., DIC).\n",
    "\n",
    "As an optional exercise, can you figure out which model specification fits the data best?\n",
    "\n",
    "In order to find this model, you should:\n",
    "\n",
    " - Change the p.map\n",
    " - Change the priors \n",
    " \n",
    "And re-run the analyses above. Don't forget to *save* the current fit (hsamples2 is most important), and *not* overwrite `/data/behavioral_data/samples.Rdata` - since you need it for model comparisons!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional resources\n",
    "The tutorial in `DMC` is very helpful: [DMC](https://osf.io/pbwx8/?view_only=6a269eca82a44ad3992b69c7a5781af5)\n",
    "\n",
    "Also, check out `hddm` if you want to do this type of stuff in Python: [HDDM](http://ski.clps.brown.edu/hddm_docs/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
